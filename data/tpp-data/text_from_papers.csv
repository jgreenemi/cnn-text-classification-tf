actively learning what makes a discrete sequence valid david janz * 1 jos van der westhuizen * 1 jose miguel hern ¬¥ andez-lobato ¬¥ * 1 abstract deep learning techniques,Not a Reddit comment
have been hugely successful for traditional supervised and unsupervised machine learning problems. in large part these techniques solve continuous optimization problems. recently however discrete generative deep learning models have been,Not a Reddit comment
successfully used to efficiently search high-dimensional discrete spaces. these methods work by representing discrete objects as sequences for which powerful sequence-based deep models can be employed. unfortunately these techniques are,Not a Reddit comment
significantly hindered by the fact that these generative models often produce invalid sequences. as a step towards solving this problem we propose to learn a deep recurrent validator model. given,Not a Reddit comment
a partial sequence our model learns the probability of that sequence occurring as the beginning of a full valid sequence. thus this identifies valid versus invalid sequences and crucially it,Not a Reddit comment
also provides insight about how individual sequence elements influence the validity of discrete objects. to learn this model we propose an approach inspired by seminal work in bayesian active learning.,Not a Reddit comment
on a synthetic dataset we demonstrate the ability of our model to distinguish valid and invalid sequences. we believe this is a key step toward learning generative models that faithfully,Not a Reddit comment
produce valid discrete objects. 1. introduction and related work generative models have seen many fascinating developments in recent years such as the ability to produce realistic images from noise (radford,Not a Reddit comment
et al. 2015) and create artwork (gatys et al. 2016). one of the most exciting research directions in generative modeling is using such models to efficiently search high-dimensional discrete spaces,Not a Reddit comment
*equal contribution 1university of cambridge uk. correspondence to: david janz <dj343@cam.ac.uk> jos van der westhuizen <jv365@cam.ac.uk>. principled approaches to deep learning international conference on machine learning sydney australia 2017. copyright,Not a Reddit comment
2017 by the author(s). (g ¬¥omez-bombarelli et al. 2016b; kusner et al. 2017). indeed discrete search is at the heart of problems in drug discovery (g ¬¥omez-bombarelli et al. 2016a),Not a Reddit comment
natural language processing (bowman et al. 2016; guimaraes et al. 2017) and symbolic regression (kusner et al. 2017). current methods for attacking these discrete search problems work by ‚Äòlifting‚Äô the,Not a Reddit comment
search from discrete space to continuous space via an autoencoder (rumelhart et al. 1985). specifically an autoencoder jointly learns two mappings: 1) a mapping from discrete space to continuous space,Not a Reddit comment
called an encoder; and 2) a reverse mapping from continuous space back to discrete space called a decoder. these mappings are learned so that if we map a discrete object,Not a Reddit comment
to a continuous one via the encoder then map it back via the decoder we reconstruct the original object. the hope is that once the autoencoder is fully trained the,Not a Reddit comment
continuous space (often called the ‚Äòlatent‚Äô space) acts as proxy for the discrete space. if this holds we can use the geometry of the continuous space to improve search using,Not a Reddit comment
(euclidean) distance measures and gradients among many other things. g ¬¥omez-bombarelli et al. (2016b) showed that is possible to use this technique to search for promising drug molecules. unfortunately these,Not a Reddit comment
methods are severely hindered by the fact that the decoder often produces invalid discrete objects. this happens because it is diffi- cult to enforce valid syntax and semantics in the,Not a Reddit comment
latent and discrete space. powerful sequential models (e.g. lstms (hochreiter & schmidhuber 1997) grus (cho et al. 2014) dcnns (kalchbrenner et al. 2014)) can exploit the relationship between parts of,Not a Reddit comment
the discrete objects (e.g. comparing similar sequences of atoms in different molecules). when employing these models as encoders and decoders generation of invalid sequences is still possible and currently this,Not a Reddit comment
happens frequently (see table 6 in the supplementary material of kusner et al. (2017)). a recent method (kusner et al. 2017) aimed to fix this by using a grammar to,Not a Reddit comment
rule out generating certain invalid sequences. however the grammar only describes syntactic constraints and cannot enforce semantic constraints. therefore certain invalid sequences can still be generated using that approach. in,Not a Reddit comment
this work-in-progress paper we propose a method for collaborative filtering using denoising auto-encoders for market basket data andres g. abad and luis i. reyes-castro escuela superior politecnica del litoral (espol),Not a Reddit comment
guayaquil-ecuador abstract recommender systems (rs) help users navigate large sets of items in the search for ‚Äúinteresting‚Äù ones. one approach to rs is collaborative filtering (cf) which is based on,Not a Reddit comment
the idea that similar users are interested in similar items. most model-based approaches to cf seek to train a machine-learning/data-mining model based on sparse data; the model is then used,Not a Reddit comment
to provide recommendations. while most of the proposed approaches are effective for small-size situations the combinatorial nature of the problem makes it impractical for medium-to-large instances. in this work we,Not a Reddit comment
present a novel approach to cf that works by training a denoising auto-encoder (dae) on corrupted baskets i.e. baskets from which one or more items have been removed. the dae,Not a Reddit comment
is then forced to learn to reconstruct the original basket given its corrupted input. due to recent advancements in optimization and other technologies for training neural-network models (such as dae),Not a Reddit comment
the proposed method results in a scalable and practical approach to cf. the contribution of this work is twofold: (1) to identify missing items in observed baskets and thus directly,Not a Reddit comment
providing a cf model; and (2) to construct a generative model of baskets which may be used for instance in simulation analysis or as part of a more complex analytical,Not a Reddit comment
method. keywords recommender systems collaborative filtering denoising auto-encoders market basket data retail firms 1. introduction recommender systems (rs) help users navigate large sets of items in the search for ‚Äúinteresting‚Äù,Not a Reddit comment
ones. in retail firms rs work by providing recommendations based on the analysis of sparse transactional data: market basket data. on-line retailers were one of the first sectors to adopt,Not a Reddit comment
rs; its application was popularized by amazon‚Äôs ‚Äúcustomers who bought this item also bought‚Äù feature [1]. in bricks-and-mortar stores rs have been successfully applied to for instance designing up-selling strategies,Not a Reddit comment
through customized discount coupons and targeted marketing campaigns. one approach to rs is collaborative filtering (cf) which is based on the idea that similar users are interested in similar items.,Not a Reddit comment
in the model-based approach to cf data is used to train a machine-learning/data-mining model. supervised-learning models proposed for cf include regression models as in [2] where a regression model was,Not a Reddit comment
proposed for predicting user‚Äôs ratings; and classification models as in [3] where an inductive approach for classification was applied. in [4] a logistic regression model together with a pca for,Not a Reddit comment
dimensionality reduction were used. other supervised-learning approaches proposed for cf include bayesian classifiers [5] and belief networks [6]. unsupervised-learning models proposed for cf include clustering techniques such as in [7],Not a Reddit comment
where k-means clustering was used. in [8] the cf problem was addressed as a sequence of decisions where the optimal policy was learned using a markov decision process (mdp); in,Not a Reddit comment
[9] latent semantic analysis was used for cf reporting higher accuracy and constant time prediction as two of the main advantages of their method. another unsupervised-learning proposed approach to cf,Not a Reddit comment
is to estimate the frequency of occurrence of combinations of items in the search of interesting association rules [10]. a direct application of this type of analysis is the quantification,Not a Reddit comment
of the complementary and supplementary relationship between items and their use for cf. from a generative-model perspective one may wish to learn the binary multivariate distribution of variables (representing the,Not a Reddit comment
presence of items in each basket) to arrive to recommendations by performing probabilistic inference. in encoding multi-resolution brain networks using unsupervised deep learning arash rahnama‚àó  abdullah alchihabi‚Ä†  vijay,Not a Reddit comment
gupta‚àó  panos j. antsaklis‚àó fatos t. yarman vural‚Ä† ‚àó department of electrical engineering university of notre dame notre dame in 46556 usa email: {arahnama vgupta2 antsaklis.1} @nd.edu. ‚Ä† department,Not a Reddit comment
of computer engineering middle east technical university 06800 ankara turkey email: abdullah.alchihabi@metu.edu.tr vural@ceng.metu.edu.tr abstract‚Äîthe main goal of this study is to extract a set of brain networks in multiple time-resolutions,Not a Reddit comment
to analyze the connectivity patterns among the anatomic regions for a given cognitive task. we suggest a deep architecture which learns the natural groupings of the connectivity patterns of human,Not a Reddit comment
brain in multiple time-resolutions. the suggested architecture is tested on task data set of human connectome project (hcp) where we extract multi-resolution networks each of which corresponds to a cognitive,Not a Reddit comment
task. at the first level of this architecture we decompose the fmri signal into multiple sub-bands using wavelet decompositions. at the second level for each sub-band we estimate a brain,Not a Reddit comment
network extracted from short time windows of the fmri signal. at the third level we feed the adjacency matrices of each mesh network at each time-resolution into an unsupervised deep,Not a Reddit comment
learning algorithm namely a stacked denoising auto-encoder (sdae). the outputs of the sdae provide a compact connectivity representation for each time window at each sub-band of the fmri signal. we,Not a Reddit comment
concatenate the learned representations of all sub-bands at each window and cluster them by a hierarchical algorithm to find the natural groupings among the windows. we observe that each cluster,Not a Reddit comment
represents a cognitive task with a performance of 93% rand index and 71% adjusted rand index. we visualize the mean values and the precisions of the networks at each component,Not a Reddit comment
of the cluster mixture. the mean brain networks at cluster centers show the variations among cognitive tasks and the precision of each cluster shows the within cluster variability of networks,Not a Reddit comment
across the subjects. index terms‚Äîdeep learning stacked autoencoders brain decoding mesh networks connectivity patterns clustering. i. introduction the data produced by functional magnetic resonance imaging (fmri) is high-dimensional and sometimes,Not a Reddit comment
not suitable for analyzing the cognitive states [1]. learning efficient lowdimensional features from high-dimensional complex input spaces is crucial for the decoding of cognitive processes. in this paper we explore,Not a Reddit comment
deep learning algorithms in order to i) find a compact representation of connectivity patterns embedded in fmri signals ii) detect natural groupings of these patterns and iii) use these natural,Not a Reddit comment
groups to extract brain networks to represent cognitive tasks. our framework is built upon our previous work in the area [2] where we decompose fmri signals into various frequency sub-bands,Not a Reddit comment
using their wavelet transforms. we further utilize the signals at different sub-bands to form multi-resolution brain networks. recent studies have shown that brain networks formed by the correlation of voxel,Not a Reddit comment
pairs‚Äô in fmri signals provide more information for brain decoding compared to the temporal information of single voxels [3] [4]. moreover there has been a shift in the literature toward,Not a Reddit comment
brain decoding algorithms that are based on the connectivity patterns in the brain motivated by the belief that these patterns provide more information about cognitive tasks than the isolated behavior,Not a Reddit comment
of individual anatomic regions [5]‚Äì[7]. contrary to the methods suggested in [3] [4] where supervised learning algorithms are employed for brain decoding in this paper we investigate the common groupings,Not a Reddit comment
in hcp task data set to find out if these natural groups correspond to the cognitive tasks. this approach enables us to find shared network representations of a cognitive task,Not a Reddit comment
together with its variations across the subjects. additionally multi-resolution representation of the fmri signals enables us to observe the variations of networks among different frequency sub-bands. after constructing the brain,Not a Reddit comment
networks representing the connectivity patterns among the anatomic regions of the brain at each sub-level a stacked de-noising auto-encoder (sdae) algorithm is employed to learn shared connectivity features associated with,Not a Reddit comment
a task based on the estimated mesh networks at different sub-bands. we concatenate the learned connectivity patterns from several wavelet sub-bands and utilize them in a hierarchical clustering algorithm with,Not a Reddit comment
a distance matrix based on their correlations. the main reason behind concatenation of the feature matrices is that the detected patterns in the brain at different frequencies provide complementary information,Not a Reddit comment
in regard to the overall cognitive state of the brain. our results show that the mesh network representation of cognitive tasks is superior compared to fmri time-series representation. we observe,Not a Reddit comment
that sdae successfully learns a set of connectivity patterns which provide an increased clustering performance. the performances are further improved by fusing the learned representations from multiple time-resolutions. this shows,Not a Reddit comment
that the modeling of the connectivity of brain in multiple sub-bands of the fmri signal leads to diverse mesh networks carrying complementary information for representing the cognitive tasks. the high,Not a Reddit comment
rand index 93% obtained at the output of the clustering algorithm proves the existence database of parliamentary speeches in ireland 1919‚Äì2013‚àó alexander herzog clemson university aherzog@clemson.edu slava j. mikhaylov university,Not a Reddit comment
of essex s.mikhaylov@essex.ac.uk august 16 2017 abstract we present a database of parliamentary debates that contains the complete record of parliamentary speeches from dail ¬¥ eireann the lower house and,Not a Reddit comment
principal chamber of ¬¥ the irish parliament from 1919 to 2013. in addition the database contains background information on all tds (teachta dala members of parliament) such as their party,Not a Reddit comment
¬¥ affiliations constituencies and office positions. the current version of the database includes close to 4.5 million speeches from 1178 tds. the speeches were downloaded from the official parliament website,Not a Reddit comment
and further processed and parsed with a python script. background information on tds was collected from the member database of the parliament website. data on cabinet positions (ministers and junior,Not a Reddit comment
ministers) was collected from the official website of the government. a record linkage algorithm and human coders were used to match tds and ministers. key words: parliamentary debates dail ¬¥,Not a Reddit comment
eireann 1 introduction almost all political decisions and political opinions are in one way or another expressed in written or spoken texts. great leaders in history become famous for their,Not a Reddit comment
ability to motivate the masses with their speeches; parties publish policy programmes before elections in order to provide information about their policy objectives; parliamentary decisions are discussed and deliberated on,Not a Reddit comment
the floor in order to exchange opinions; members of the executive in most political systems are legally obliged to provide written or verbal answers to questions from legislators; and citizens,Not a Reddit comment
express their opinions about political events on internet blogs or in public online chats. political texts and speeches are everywhere that people express their political opinions and preferences. it is,Not a Reddit comment
not until recently that social scientists have discovered the potential of analyzing political texts to test theories of political behavior. one reason is that systematically processing large quantities of textual,Not a Reddit comment
data to retrieve information is technically challenging. computational advances in natural language processing have greatly facilitated this task. adaptation of such techniques in social science ‚Äì for example wordscore (benoit,Not a Reddit comment
and laver 2003; laver benoit and garry 2003) or wordfish (slapin and proksch 2008) ‚Äì now enable researchers to systematically compare documents with one another and extract relevant information from,Not a Reddit comment
them. applied to party manifestos for which most of these techniques have been developed these methods can be used to evaluate the similarity or dissimilarity between manifestos which can then,Not a Reddit comment
be used to derive estimates about parties‚Äô policy preferences and their ideological distance to each other. one area of research that increasingly makes use of quantitative text methods are studies,Not a Reddit comment
of legislative behavior and parliaments (giannetti and laver 2005; laver and benoit the trimmed lasso: sparsity and robustness dimitris bertsimas martin s. copenhaver and rahul mazumder‚àó august 15 2017 abstract,Not a Reddit comment
nonconvex penalty methods for sparse modeling in linear regression have been a topic of fervent interest in recent years. herein we study a family of nonconvex penalty functions that we,Not a Reddit comment
call the trimmed lasso and that offers exact control over the desired level of sparsity of estimators. we analyze its structural properties and in doing so show the following: 1.,Not a Reddit comment
drawing parallels between robust statistics and robust optimization we show that the trimmed-lasso-regularized least squares problem can be viewed as a generalized form of total least squares under a specific,Not a Reddit comment
model of uncertainty. in contrast this same model of uncertainty viewed instead through a robust optimization lens leads to the convex slope (or owl) penalty. 2. further in relating the,Not a Reddit comment
trimmed lasso to commonly used sparsity-inducing penalty functions we provide a succinct characterization of the connection between trimmed-lassolike approaches and penalty functions that are coordinate-wise separable showing that the trimmed,Not a Reddit comment
penalties subsume existing coordinate-wise separable penalties with strict containment in general. 3. finally we describe a variety of exact and heuristic algorithms both existing and new for trimmed lasso regularized,Not a Reddit comment
estimation problems. we include a comparison between the different approaches and an accompanying implementation of the algorithms. 1 introduction sparse modeling in linear regression has been a topic of fervent,Not a Reddit comment
interest in recent years [23 42]. this interest has taken several forms from substantial developments in the theory of the lasso to advances in algorithms for convex optimization. throughout there,Not a Reddit comment
has been a strong emphasis on the increasingly high-dimensional nature of linear regression problems; in such problems where the number of variables p can vastly exceed the number of observations,Not a Reddit comment
n sparse modeling techniques are critical for performing inference. context one of the fundamental approaches to sparse modeling in the usual linear regression model of y = xŒ≤ + ,Not a Reddit comment
with y ‚àà r n and x ‚àà r n√óp  is the best subset selection [57] problem: theoretical foundation of co-training and disagreement-based algorithms wei wang zhi-hua zhou‚àó national,Not a Reddit comment
key laboratory for novel software technology nanjing university nanjing 210023 china abstract disagreement-based approaches generate multiple classifiers and exploit the disagreement among them with unlabeled data to improve learning performance.,Not a Reddit comment
co-training is a representative paradigm of them which trains two classifiers separately on two sufficient and redundant views; while for the applications where there is only one view several successful,Not a Reddit comment
variants of co-training with two different classifiers on single-view data instead of two views have been proposed. for these disagreement-based approaches there are several important issues which still are unsolved,Not a Reddit comment
in this article we present theoretical analyses to address these issues which provides a theoretical foundation of co-training and disagreement-based approaches. keywords: machine learning semi-supervised learning disagreement-based learning co-training multi-view,Not a Reddit comment
classification combination 1. introduction learning from labeled training data is well-established in traditional machine learning but labeling the data is time-consuming sometimes may be very expensive since it requires human,Not a Reddit comment
efforts. in many practical applications unlabeled data can be obtained abundantly and cheaply. for example in the task of graph classification via deep learning with virtual nodes trang pham\ ,Not a Reddit comment
truyen tran\  hoa dam[  svetha venkatesh\ \centre for pattern recognition and data analytics deakin university geelong australia {phtra truyen.tran svetha.venkatesh}@deakin.edu.au; [school of computing and information technology university of,Not a Reddit comment
wollongong australia hoa@uow.edu.au abstract learning representation for graph classification turns a variable-size graph into a fixed-size vector (or matrix). such a representation works nicely with algebraic manipulations. here we introduce,Not a Reddit comment
a simple method to augment an attributed graph with a virtual node that is bidirectionally connected to all existing nodes. the virtual node represents the latent aspects of the graph,Not a Reddit comment
which are not immediately available from the attributes and local connectivity structures. the expanded graph is then put through any node representation method. the representation of the virtual node is,Not a Reddit comment
then the representation of the entire graph. in this paper we use the recently introduced column network for the expanded graph resulting in a new end-to-end graph classification model dubbed,Not a Reddit comment
virtual column network (vcn). the model is validated on two tasks: (i) predicting bioactivity of chemical compounds and (ii) finding software vulnerability from source code. results demonstrate that vcn is,Not a Reddit comment
competitive against wellestablished rivals. 1 intro deep learning has achieved record-breaking successes in domains with regular grid (e.g. via cnn) or chain-like (e.g. via rnn) structures [lecun et al. 2015].,Not a Reddit comment
however many if not most real-world structured problems are best modelled using graphs with irregular connectivity. these include for example chemical compounds proteins rnas function calls in software brain activity,Not a Reddit comment
networks and social networks. a canonical task is graph classification that is assigning class labels to a graph instance (e.g. chemical compound as its activity against cancer cells). we study,Not a Reddit comment
a generic class known as attributed graphs whose nodes can have attributes and edges can be multi-typed and directed. we aim to efficiently learn distributed representation of graph that is,Not a Reddit comment
a map that turns variable-size graphs into fixed-size vectors or matrices. such a representation would benefit greatly from a powerful pool of data manipulation tools. this rules out traditional approaches,Not a Reddit comment
such as graph kernels [vishwanathan et al. 2010] and graph feature engineering [choetkiertikul et al. 2017] which could be either computation or labor intensive. several recent neural networks defined on,Not a Reddit comment
graphs such as graph neural network [scarselli et al. 2009] diffusion-cnn [atwood and towsley 2016] and column network [pham et al. 2017a] start with node representations by taking into account,Not a Reddit comment
of the neighborhood structures typically through convolution and/or recurrent operations. node representations can then be aggregated into graph representation. it is akin to representing a document1 by first embedding words,Not a Reddit comment
into vectors (e.g. through word2vec) then combining them (e.g. by weighted averaging using attention). we conjecture that a better way is to learn graph representation directly and simultaneously with node,Not a Reddit comment
representation2 . our main idea is to augment the original graph with a virtual node to represent the latent aspects of the graph. the virtual node is bidirectionally connected to,Not a Reddit comment
all existing real nodes. the virtual node assumes either empty attributes or auxiliary information which are not readily available in the original graph. the augmented-graph is passed through the existing,Not a Reddit comment
graph neural networks for computing node representation. the graph representation is then a vector representation of the virtual node. for concreteness we materialize the idea of virtual node using column,Not a Reddit comment
network [pham et al. 2017a] an architecture for node classification. with a differentiable classifier (e.g. a feedforward net) the network is an end-to-end solution for graph classification which we name,Not a Reddit comment
virtual column network (vcn). we validate the vcn on two applications: predicting bio-activity of chemical compounds and assessing vulnerability in software code and the results are promising. 2 models in,Not a Reddit comment
this section we present virtual column network (vcn) a realization of the idea of virtual node for graph classification using a recent node representation method known as column network (cln),Not a Reddit comment
[pham et al. 2017a]. vcn is applicable to graphs of multi-typed edges and attributed nodes. 1a document can be considered as a linear graph of words. 2this is akin to,Not a Reddit comment
the spirit of paragraph2vec [le and mikolov fine-gray competing risks model with high-dimensional covariates: estimation and inference jue hou1  jelena bradic1 and ronghui xu12 1department of mathematics 2department of,Not a Reddit comment
family medicine and public health university of california san diego ca 92093 abstract the purpose of this paper is to construct confidence intervals for the regression coefficients in the fine-gray,Not a Reddit comment
model for competing risks data with random censoring where the number of covariates can be larger than the sample size. despite strong motivation from biostatistics applications highdimensional fine-gray model has,Not a Reddit comment
attracted relatively little attention among the methodological or theoretical literatures. we fill in this blank by proposing first a consistent regularized estimator and then the confidence intervals based on the,Not a Reddit comment
one-step bias-correcting estimator. we are able to generalize the partial likelihood approach for the fine-gray model under random censoring despite many technical difficulties. we lay down a methodological and theoretical,Not a Reddit comment
framework for the one-step bias-correcting estimator with the partial likelihood which does not have independent and identically distributed entries. we also handle for our theory the approximation error from the,Not a Reddit comment
inverse probability weighting (ipw) proposing novel concentration results for time dependent processes. in addition to the theoretical results and algorithms we present extensive numerical experiments and an application to a,Not a Reddit comment
study of non-cancer mortality among prostate cancer patients using the linked medicare-seer data. 1 introduction in many applications we want to use data to draw inferences about the effect of,Not a Reddit comment
a covariate on a specific event in the presence of many risks competing for the same event: examples include medical studies about the effect of a medical treatment on health,Not a Reddit comment
outcomes of chronically ill patients studies of the unemployment duration and transitions to employment and labor market programs or evaluations of environmental determinants of child mortality and studying internetwork competition,Not a Reddit comment
risk ‚Äústrategic gridlock‚Äù a study of how firms use alliances to respond to the alliance networks of their rivals. historically most datasets have been too small to meaningfully explore heterogeneity,Not a Reddit comment
between different risk factors beyond considering cause-specific models only. recently however there has been an explosion of experimental data sets where it is potentially feasible to develop estimates in full,Not a Reddit comment
competing risks models. high-dimensional regression has attracted increased interest in statistical analysis and has provided a useful tool in modern biomedical ecological astrophysical or economics data pertaining to setting where,Not a Reddit comment
the number of parameters is greater than the number of samples (see b¬®uhlmann and van de geer (2011) for an overview). regularized methods (fan and li 2001; tibshirani 1996) provide,Not a Reddit comment
straightforward interpretation of resulting estimators while allowing the number of covariates to be exponentially larger than the sample size. considerable research effort has been devoted to developing regularized methods to,Not a Reddit comment
handle various regression settings (ravikumar et al. 2010; belloni and chernozhukov 2011; obozinski et al. 2011; meinshausen and b¬®uhlmann 2006; basu and michailidis 2015; cho and fryzlewicz 2015) including those,Not a Reddit comment
for time-to-event data (sun et al. (2014); bradic et al. (2011); ga¬®ƒ±ffas and guilloux (2012); johnson (2008); lemler (2013); bradic and song (2015); huang et al. (2006); among others). however,Not a Reddit comment
regression has not been studied for the competing risks setting a scenario frequently encountered in practice with random censoring and high-dimensional covariates. as an illustration project of how information contained,Not a Reddit comment
in patients‚Äô electronic medical records can be harvested for the purposes of precision medicine we consider the data set linking the surveillance epidemiology and end results (seer) program database of,Not a Reddit comment
the national cancer institute with the federal health insurance program medicare database for prostate cancer patients of age 65 or older. when restricted to patients diagnosed between 2004 and 2009,Not a Reddit comment
in the seermedicare database after excluding additional patients with missing clinical records we have a total of 57011 patients who have information available on 7 relevant clinical variables (age psa,Not a Reddit comment
gleason score ajcc stage and ajcc stage t n m respectively) 5 demographical variables (race marital status metro registry and year of diagnosis) plus 8971 binary insurance claim codes. until,Not a Reddit comment
december 2013 (end of follow-up for this data) there were a total of 1247 deaths due to cancer and 5221 deaths unrelated to cancer. the goal of this paper is,Not a Reddit comment
to develop methodology for the fine-gray model with many more covariates than the number of events which can be used to appropriately and flexibly evaluate the impact of risk factors,Not a Reddit comment
on the non-cancer versus cancer mortality as reflected in these clinical demographical and claim codes which indirectly describe events that occur in surgical procedures hospitalization and outpatient activities. this understanding,Not a Reddit comment
will then in turn aid in clinical decision making as to whether pursue aggressive cancer-directed therapy in the presence of pre-existing comorbidities. there are at least three major challenges for,Not a Reddit comment
addressing high-dimensional competing risks regression under the fine-gray model which directly associates the risk factors with the cumulative incidence function of a particular cause. the structure of the score function,Not a Reddit comment
related to the partial likelihood is a rather subtle issue with many of the unobserved factors ruining a simple martin- a projection pursuit framework for testing general high-dimensional hypothesis yinchu,Not a Reddit comment
zhu rady school of management university of california san diego and jelena bradic ‚àó department of mathematics university of california san diego may 2 2017 abstract this article develops a,Not a Reddit comment
framework for testing general hypothesis in high-dimensional models where the number of variables may far exceed the number of observations. existing literature has considered less than a handful of hypotheses,Not a Reddit comment
such as testing individual coordinates of the model parameter. however the problem of testing general and complex hypotheses remains widely open. we propose a new inference method developed around the,Not a Reddit comment
hypothesis adaptive projection pursuit framework which solves the testing problems in the most general case. the proposed inference is centered around a new class of estimators defined as l1 projection,Not a Reddit comment
of the initial guess of the unknown onto the space defined by the null. this projection automatically takes into account the structure of the null hypothesis and allows us to,Not a Reddit comment
study formal inference for a number of longstanding problems. for example we can directly conduct inference on the sparsity level of the model parameters and the minimum signal strength. this,Not a Reddit comment
is especially significant given the fact that the former is a fundamental condition underlying most of the theoretical development in high-dimensional statistics while the latter is a key condition used,Not a Reddit comment
to establish variable selection properties. moreover the proposed method is asymptotically exact and has satisfactory power properties for testing very general functionals of the high-dimensional parameters. the simulation studies lend,Not a Reddit comment
further support to our theoretical claims and additionally show excellent finite-sample size and power properties of the proposed test. 1 introduction high-dimensional statistical inference of models in which the number,Not a Reddit comment
of variables p might be much larger than the sample size n (i.e. p  n) has become a fundamental issue in many areas of applications. examples include image analysis,Not a Reddit comment
analysis of high-throughput genomic sequences speech analysis etc where the hdlss (high-dimensional low sample size with p/n ‚Üí ‚àû) data structure is apparent. the goal of many empirical analyses is,Not a Reddit comment
to understand the parameter structure of the model at hand hence developing methodology that is flexible and broad-ranging as far as the hypotheses are concerned is of great practical importance.,Not a Reddit comment
our goal is to construct a test that can perform well for a large class of high-dimensional null hypothesis. in this paper we consider a general model based on n,Not a Reddit comment
independent and identically distributed observations z1 z2 . . .  zn of a random variable (vector) z with support z a loss function l(¬∑ ¬∑) : z √ó b,Not a Reddit comment
‚Üí r a parameter space b ‚äÜ r p and a true parameter value defined as Œ≤‚àó = arg min Œ≤‚ààb l(Œ≤) (1.1) with l(Œ≤) = e[l(z Œ≤)]. the above,Not a Reddit comment
formulation covers many important statistical models; for the parametric likelihood models z is generated from a distribution pŒ≤ with the true value Œ≤‚àó defined by (1.1) with l(z Œ≤) =,Not a Reddit comment
‚àí log pŒ≤(z) where pŒ≤(¬∑) is the probability density function corresponding to pŒ≤. the main goal of this paper is to fill in the gap in the current high-dimensional literature,Not a Reddit comment
and present a comprehensive methodology for the following testing problem h0 : Œ≤‚àó ‚àà b0 vs h1 : Œ≤‚àó ‚àà b / 0 (1.2) for a given set b0 ‚äÇ,Not a Reddit comment
b. here no restrictions such as convexity or dimensionality are imposed on the set b0. explicitly we would like to design a test that is asymptotically exact irrespective of the,Not a Reddit comment
geometry of the set b0. this problem can be motivated first as a high-level approach to performing inference in highdimensions for complex hypothesis. since p  n further assumptions such,Not a Reddit comment
is sparsity have been naturally exploited. there one assumes that the number of non-zeros of Œ≤‚àó kŒ≤‚àók0 is smaller than n. despite the fact that hypothesis testing problem (1.2) is,Not a Reddit comment
a difficult problem it is naturally related to a number of important questions. example 1.1 (testing the sparsity level). over the past decade sparsity assumption has become two-sample testing in,Not a Reddit comment
non-sparse high-dimensional linear models yinchu zhu and jelena bradic rady school of management university of california san diego la jolla california 92093 usa e-mail: yinchu.zhu@rady.ucsd.edu department of mathematics university of,Not a Reddit comment
california san diego la jolla california 92093 usa e-mail: jbradic@math.ucsd.edu abstract: in analyzing high-dimensional models sparsity of the model parameter is a common but often undesirable assumption. though different methods,Not a Reddit comment
have been proposed for hypothesis testing under sparsity no systematic theory exists for inference methods that are robust to failure of the sparsity assumption. in this paper we study the,Not a Reddit comment
following two-sample testing problem: given two samples generated by two high-dimensional linear models we aim to test whether the regression coefficients of the two linear models are identical. we propose,Not a Reddit comment
a framework named tiers (short for testing equality of regression slopes) which solves the two-sample testing problem without making any assumptions on the sparsity of the regression parameters. tiers builds,Not a Reddit comment
a new model by convolving the two samples in such a way that the original hypothesis translates into a new moment condition. a self-normalization construction is then developed to form,Not a Reddit comment
a moment test. we provide rigorous theory for the developed framework. under very weak conditions of the feature covariance we show that the accuracy of the proposed test in controlling,Not a Reddit comment
type i errors is robust both to the lack of sparsity in the features and to the heavy tails in the error distribution even when the sample size is much,Not a Reddit comment
smaller than the feature dimension. moreover we discuss minimax optimality and efficiency properties of the proposed test. simulation analysis demonstrates excellent finite-sample performance of our test. in deriving the test,Not a Reddit comment
we also develop tools that are of independent interest. the test is built upon a novel estimator called auto-adaptive dantzig selector (adds) which not only automatically chooses an appropriate scale,Not a Reddit comment
(variance) of the error term but also incorporates prior information. to effectively approximate the critical value of the test statistic we develop a novel high-dimensional plug-in approach that complements the,Not a Reddit comment
recent advances in gaussian approximation theory. 1. introduction high-dimensional data are increasingly encountered in many applications of statistics and most prominently in biological and financial research. a common feature of,Not a Reddit comment
the statistical models used to study high-dimensional data is robust confidence intervals in high-dimensional left-censored regression jelena bradic and jiaqi guo abstract. this paper develops robust confidence intervals in high-dimensional,Not a Reddit comment
and left-censored regression. type-i censored regression models are extremely common in practice where a competing event makes the variable of interest unobservable. however techniques developed for entirely observed data do,Not a Reddit comment
not directly apply to the censored observations. in this paper we develop smoothed estimating equations that augment the de-biasing method such that the resulting estimator is adaptive to censoring and,Not a Reddit comment
is more robust to the misspecification of the error distribution. we propose a unified class of robust estimators including mallow‚Äôs schweppe‚Äôs and hill-ryan‚Äôs one-step estimator. in the ultra-high-dimensional setting where,Not a Reddit comment
the dimensionality can grow exponentially with the sample size we show that as long as the preliminary estimator converges faster than n ‚àí1/4  the one-step estimator inherits asymptotic distribution,Not a Reddit comment
of fully iterated version. moreover we show that the size of the residuals of the bahadur representation matches those of the simple linear models s 3/4 (log(p ‚à® n))3/4/n1/4 ‚Äì,Not a Reddit comment
that is the effects of censoring asymptotically disappear. simulation studies demonstrate that our method is adaptive to the censoring level and asymmetry in the error distribution and does not lose,Not a Reddit comment
efficiency when the errors are from symmetric distributions. finally we apply the developed method to a real data set from the maqc-ii repository that is related to the hiv-1 study.,Not a Reddit comment
1. introduction left-censored data is a characteristic of many datasets. in physical science applications observations can be censored due to limit of detection and quantifi- cation in the measurements. for,Not a Reddit comment
example if a measurement device has a value limit on the lower end the observations is recorded with the minimum value even though the actual result is below the measurement,Not a Reddit comment
range. in fact many of the hiv studies have to deal with difficulties due to the lower quantification and detection limits of viral load assays [30]. in social science studies,Not a Reddit comment
censoring may be implied in the nonnegative nature or defined through human actions. economic policies such as minimum wage and minimum transaction fee result in left-censored data as quantities below,Not a Reddit comment
